{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import gc\n",
    "import json\n",
    "from halo import Halo\n",
    "from numerapi import NumerAPI\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from utils import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    neutralize,\n",
    "    get_biggest_change_features,\n",
    "    validation_metrics,\n",
    "    get_time_series_cross_val_splits,\n",
    "    save_model_config,\n",
    "    load_model_config,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    "    EXAMPLE_PREDS_COL\n",
    ")\n",
    "napi = NumerAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All training and model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model_configs = {\n",
    "    \"LGBM_cfg1\": {\n",
    "        \"n_estimators\": 2000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"max_depth\": 5,\n",
    "        \"num_leaves\": 2 ** 5,\n",
    "        \"colsample_bytree\": 0.1\n",
    "    },\n",
    "    \"LGBM_cfg2\": {\n",
    "        \"n_estimators\": 3000,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"max_depth\": 5,\n",
    "        \"num_leaves\": 2 ** 5,\n",
    "        \"colsample_bytree\": 0.1\n",
    "    },\n",
    "    \"RF_cfg1\": {\n",
    "        \"n_estimators\": 1000,\n",
    "        \"criterion\": \"squared_error\",\n",
    "        \"max_depth\": 5,\n",
    "        \"min_samples_leaf\": 5\n",
    "    },\n",
    "    \"RF_cfg2\": {\n",
    "        \"n_estimators\": 2000,\n",
    "        \"criterion\": \"squared_error\",\n",
    "        \"max_depth\": 3,\n",
    "        \"min_samples_leaf\": 5\n",
    "    },\n",
    "    \"XGB_cfg1\": {\n",
    "        \"n_estimators\": 2000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"max_depth\": 5,\n",
    "        \"colsample_bytree\": 0.1\n",
    "    },\n",
    "    \"RF_cfg2\": {\n",
    "        \"n_estimators\": 3000,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"max_depth\": 5,\n",
    "        \"colsample_bytree\": 0.1\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "training_configs = {\n",
    "    \"FEATURE_SET\": \"small\",\n",
    "    \"MODEL_CONFIG\": \"LGBM_cfg1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament\n",
    "# napi.download_dataset(\"numerai_tournament_data.parquet\", f\"data/tournament_data_{current_round}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "spinner = Halo(text='', spinner='dots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature set information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "with open(\"data/features.json\", \"r\") as f:\n",
    "    feature_metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "feature_set = training_configs[\"FEATURE_SET\"]\n",
    "features = feature_metadata[\"feature_sets\"][feature_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "len(feature_metadata[\"feature_sets\"][\"small\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['legacy', 'small', 'medium'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "feature_metadata[\"feature_sets\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# read in just those features along with era and target columns\n",
    "read_columns = features + [ERA_COL, DATA_TYPE_COL, TARGET_COL]\n",
    "training_data = pd.read_parquet('data/numerai_training_data.parquet', columns=read_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50    1206036\n",
       "0.75     482458\n",
       "0.25     482411\n",
       "0.00     120613\n",
       "1.00     120587\n",
       "Name: target_nomi_20, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "training_data[\"target_nomi_20\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model_config = training_configs[\"MODEL_CONFIG\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Regression models (LGBM, RF, XGBOOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'model_medium_LGBM_cfg1'\n"
     ]
    }
   ],
   "source": [
    "model_name = \"model_{}_{}\".format(feature_set, model_config)\n",
    "print(f\"Checking for existing model '{model_name}'\")\n",
    "model = load_model(model_name)\n",
    "if not model:\n",
    "    print(f\"model not found, creating new one\")\n",
    "    params = model_configs[model_config]\n",
    "\n",
    "    if model_config.startswith(\"LGBM\"):\n",
    "        model = LGBMRegressor(**params)\n",
    "    elif model_config.startswith(\"RF\"):\n",
    "        model = RandomForestRegressor(**params)\n",
    "    else:\n",
    "        model = XGBRegressor(**params)\n",
    "\n",
    "    # train on all of train and save the model so we don't have to train next time\n",
    "    spinner.start('Training model')\n",
    "    model.fit(training_data.filter(like='feature_', axis='columns'),\n",
    "              training_data[TARGET_COL])\n",
    "    print(f\"saving new model: {model_name}\")\n",
    "    save_model(model, model_name)\n",
    "    spinner.succeed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading minimal features of validation and tournament data...\n"
     ]
    }
   ],
   "source": [
    "print('Reading minimal features of validation and tournament data...')\n",
    "validation_data = pd.read_parquet('data/numerai_validation_data.parquet',\n",
    "                                  columns=read_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tournament_data = pd.read_parquet(f'data/tournament_data_{current_round}.parquet',\n",
    "                                  columns=read_columns)\n",
    "nans_per_col = tournament_data[tournament_data[\"data_type\"] == \"live\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nans per column this week: target_nomi_20    5384\n",
      "dtype: int64\n",
      "out of 5384 total rows\n",
      "filling nans with 0.5\n"
     ]
    }
   ],
   "source": [
    "# check for nans and fill nans\n",
    "if nans_per_col.any():\n",
    "    total_rows = len(tournament_data[tournament_data[\"data_type\"] == \"live\"])\n",
    "    print(f\"Number of nans per column this week: {nans_per_col[nans_per_col > 0]}\")\n",
    "    print(f\"out of {total_rows} total rows\")\n",
    "    print(f\"filling nans with 0.5\")\n",
    "    tournament_data.loc[:, features].fillna(0.5, inplace=True)\n",
    "else:\n",
    "    print(\"No nans in the features this week!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Predicting on validation and tournament data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "spinner.start('Predicting on validation and tournament data')\n",
    "# double check the feature that the model expects vs what is available to prevent our\n",
    "# pipeline from failing if Numerai adds more data and we don't have time to retrain!\n",
    "model_expected_features = model.booster_.feature_name()\n",
    "if set(model_expected_features) != set(features):\n",
    "    print(f\"New features are available! Might want to retrain model {model_name}.\")\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(\n",
    "    validation_data.loc[:, model_expected_features])\n",
    "tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(\n",
    "    tournament_data.loc[:, model_expected_features])\n",
    "spinner.succeed()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "all_feature_corrs = training_data.groupby(ERA_COL).apply(\n",
    "    lambda era: era[features].corrwith(era[TARGET_COL])\n",
    ")\n",
    "riskiest_features = get_biggest_change_features(all_feature_corrs, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Neutralizing to risky features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<halo.halo.Halo at 0x1231d3340>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "spinner.start('Neutralizing to risky features')\n",
    "\n",
    "# neutralize our predictions to the riskiest features\n",
    "validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n",
    "    df=validation_data,\n",
    "    columns=[f\"preds_{model_name}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n",
    "\n",
    "tournament_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n",
    "    df=tournament_data,\n",
    "    columns=[f\"preds_{model_name}\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n",
    "spinner.succeed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "current_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\n",
    "\n",
    "# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\n",
    "validation_data[\"prediction\"].to_csv(f\"pred/validation_predictions_{model_name}_{current_round}.csv\")\n",
    "tournament_data[\"prediction\"].to_csv(f\"pred/tournament_predictions_{model_name}_{current_round}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model_to_submit = f\"preds_{model_name}\"\n",
    "\n",
    "validation_data[\"prediction_w_risk\"] = validation_data[model_to_submit].rank(pct=True)\n",
    "tournament_data[\"prediction_w_risk\"] = tournament_data[model_to_submit].rank(pct=True)\n",
    "validation_data[\"prediction_w_risk\"].to_csv(f\"pred/validation_predictions_{model_name}_{current_round}.csv\")\n",
    "tournament_data[\"prediction_w_risk\"].to_csv(f\"pred/tournament_predictions_{model_name}_{current_round}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\n",
    "\n",
    "# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\n",
    "validation_data[\"prediction\"].to_csv(f\"pred/validation_predictions_{model_name}_{current_round}.csv\")\n",
    "tournament_data[\"prediction\"].to_csv(f\"pred/tournament_predictions_{model_name}_{current_round}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_dichasial_hammier_spawner</th>\n",
       "      <th>feature_rheumy_epistemic_prancer</th>\n",
       "      <th>feature_pert_performative_hormuz</th>\n",
       "      <th>feature_hillier_unpitied_theobromine</th>\n",
       "      <th>feature_perigean_bewitching_thruster</th>\n",
       "      <th>feature_renegade_undomestic_milord</th>\n",
       "      <th>feature_koranic_rude_corf</th>\n",
       "      <th>feature_demisable_expiring_millepede</th>\n",
       "      <th>feature_unscheduled_malignant_shingling</th>\n",
       "      <th>feature_clawed_unwept_adaptability</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_dipped_sent_giuseppe</th>\n",
       "      <th>feature_undivorced_unsatisfying_praetorium</th>\n",
       "      <th>feature_reclinate_cruciform_lilo</th>\n",
       "      <th>era</th>\n",
       "      <th>data_type</th>\n",
       "      <th>target_nomi_20</th>\n",
       "      <th>preds_model_medium_LGBM_cfg1</th>\n",
       "      <th>preds_model_medium_LGBM_cfg1_neutral_riskiest_50</th>\n",
       "      <th>prediction</th>\n",
       "      <th>example_preds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n000777698096000</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0857</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.475515</td>\n",
       "      <td>-1.015812</td>\n",
       "      <td>0.141486</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n0009793a3b91c27</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0857</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.509630</td>\n",
       "      <td>0.296737</td>\n",
       "      <td>0.623966</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n00099ccd6698ab0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0857</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.517587</td>\n",
       "      <td>0.562613</td>\n",
       "      <td>0.725224</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n0019e36bbb8702b</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0857</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.507401</td>\n",
       "      <td>0.124685</td>\n",
       "      <td>0.552808</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n0028cb874439df8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0857</td>\n",
       "      <td>validation</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.486497</td>\n",
       "      <td>-0.633783</td>\n",
       "      <td>0.247748</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 427 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature_dichasial_hammier_spawner  \\\n",
       "id                                                    \n",
       "n000777698096000                               0.50   \n",
       "n0009793a3b91c27                               0.75   \n",
       "n00099ccd6698ab0                               0.25   \n",
       "n0019e36bbb8702b                               0.50   \n",
       "n0028cb874439df8                               0.00   \n",
       "\n",
       "                  feature_rheumy_epistemic_prancer  \\\n",
       "id                                                   \n",
       "n000777698096000                              0.50   \n",
       "n0009793a3b91c27                              0.25   \n",
       "n00099ccd6698ab0                              0.75   \n",
       "n0019e36bbb8702b                              1.00   \n",
       "n0028cb874439df8                              0.75   \n",
       "\n",
       "                  feature_pert_performative_hormuz  \\\n",
       "id                                                   \n",
       "n000777698096000                              0.25   \n",
       "n0009793a3b91c27                              0.50   \n",
       "n00099ccd6698ab0                              0.00   \n",
       "n0019e36bbb8702b                              0.25   \n",
       "n0028cb874439df8                              0.00   \n",
       "\n",
       "                  feature_hillier_unpitied_theobromine  \\\n",
       "id                                                       \n",
       "n000777698096000                                  0.25   \n",
       "n0009793a3b91c27                                  0.75   \n",
       "n00099ccd6698ab0                                  0.75   \n",
       "n0019e36bbb8702b                                  0.75   \n",
       "n0028cb874439df8                                  0.25   \n",
       "\n",
       "                  feature_perigean_bewitching_thruster  \\\n",
       "id                                                       \n",
       "n000777698096000                                  0.00   \n",
       "n0009793a3b91c27                                  1.00   \n",
       "n00099ccd6698ab0                                  1.00   \n",
       "n0019e36bbb8702b                                  0.75   \n",
       "n0028cb874439df8                                  1.00   \n",
       "\n",
       "                  feature_renegade_undomestic_milord  \\\n",
       "id                                                     \n",
       "n000777698096000                                0.00   \n",
       "n0009793a3b91c27                                0.00   \n",
       "n00099ccd6698ab0                                0.75   \n",
       "n0019e36bbb8702b                                0.50   \n",
       "n0028cb874439df8                                0.50   \n",
       "\n",
       "                  feature_koranic_rude_corf  \\\n",
       "id                                            \n",
       "n000777698096000                       0.00   \n",
       "n0009793a3b91c27                       0.25   \n",
       "n00099ccd6698ab0                       1.00   \n",
       "n0019e36bbb8702b                       0.00   \n",
       "n0028cb874439df8                       0.00   \n",
       "\n",
       "                  feature_demisable_expiring_millepede  \\\n",
       "id                                                       \n",
       "n000777698096000                                  0.75   \n",
       "n0009793a3b91c27                                  0.25   \n",
       "n00099ccd6698ab0                                  1.00   \n",
       "n0019e36bbb8702b                                  0.75   \n",
       "n0028cb874439df8                                  0.00   \n",
       "\n",
       "                  feature_unscheduled_malignant_shingling  \\\n",
       "id                                                          \n",
       "n000777698096000                                     1.00   \n",
       "n0009793a3b91c27                                     1.00   \n",
       "n00099ccd6698ab0                                     0.75   \n",
       "n0019e36bbb8702b                                     0.00   \n",
       "n0028cb874439df8                                     0.00   \n",
       "\n",
       "                  feature_clawed_unwept_adaptability  ...  \\\n",
       "id                                                    ...   \n",
       "n000777698096000                                0.25  ...   \n",
       "n0009793a3b91c27                                0.25  ...   \n",
       "n00099ccd6698ab0                                1.00  ...   \n",
       "n0019e36bbb8702b                                0.25  ...   \n",
       "n0028cb874439df8                                0.50  ...   \n",
       "\n",
       "                  feature_dipped_sent_giuseppe  \\\n",
       "id                                               \n",
       "n000777698096000                          1.00   \n",
       "n0009793a3b91c27                          0.50   \n",
       "n00099ccd6698ab0                          1.00   \n",
       "n0019e36bbb8702b                          0.75   \n",
       "n0028cb874439df8                          1.00   \n",
       "\n",
       "                  feature_undivorced_unsatisfying_praetorium  \\\n",
       "id                                                             \n",
       "n000777698096000                                        0.50   \n",
       "n0009793a3b91c27                                        0.25   \n",
       "n00099ccd6698ab0                                        0.25   \n",
       "n0019e36bbb8702b                                        0.25   \n",
       "n0028cb874439df8                                        1.00   \n",
       "\n",
       "                  feature_reclinate_cruciform_lilo   era   data_type  \\\n",
       "id                                                                     \n",
       "n000777698096000                               1.0  0857  validation   \n",
       "n0009793a3b91c27                               0.5  0857  validation   \n",
       "n00099ccd6698ab0                               1.0  0857  validation   \n",
       "n0019e36bbb8702b                               1.0  0857  validation   \n",
       "n0028cb874439df8                               1.0  0857  validation   \n",
       "\n",
       "                  target_nomi_20  preds_model_medium_LGBM_cfg1  \\\n",
       "id                                                               \n",
       "n000777698096000            0.25                      0.475515   \n",
       "n0009793a3b91c27            0.50                      0.509630   \n",
       "n00099ccd6698ab0            0.00                      0.517587   \n",
       "n0019e36bbb8702b            0.50                      0.507401   \n",
       "n0028cb874439df8            0.50                      0.486497   \n",
       "\n",
       "                  preds_model_medium_LGBM_cfg1_neutral_riskiest_50  \\\n",
       "id                                                                   \n",
       "n000777698096000                                         -1.015812   \n",
       "n0009793a3b91c27                                          0.296737   \n",
       "n00099ccd6698ab0                                          0.562613   \n",
       "n0019e36bbb8702b                                          0.124685   \n",
       "n0028cb874439df8                                         -0.633783   \n",
       "\n",
       "                  prediction  example_preds  \n",
       "id                                           \n",
       "n000777698096000    0.141486            NaN  \n",
       "n0009793a3b91c27    0.623966            NaN  \n",
       "n00099ccd6698ab0    0.725224            NaN  \n",
       "n0019e36bbb8702b    0.552808            NaN  \n",
       "n0028cb874439df8    0.247748            NaN  \n",
       "\n",
       "[5 rows x 427 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction_w_risk</th>\n",
       "      <th>prediction</th>\n",
       "      <th>preds_model_medium_LGBM_cfg1</th>\n",
       "      <th>preds_model_medium_LGBM_cfg1_neutral_riskiest_50</th>\n",
       "      <th>target_nomi_20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n000777698096000</th>\n",
       "      <td>0.026383</td>\n",
       "      <td>0.141486</td>\n",
       "      <td>0.475515</td>\n",
       "      <td>-1.015812</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n0009793a3b91c27</th>\n",
       "      <td>0.766374</td>\n",
       "      <td>0.623966</td>\n",
       "      <td>0.509630</td>\n",
       "      <td>0.296737</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n00099ccd6698ab0</th>\n",
       "      <td>0.906050</td>\n",
       "      <td>0.725224</td>\n",
       "      <td>0.517587</td>\n",
       "      <td>0.562613</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n0019e36bbb8702b</th>\n",
       "      <td>0.711680</td>\n",
       "      <td>0.552808</td>\n",
       "      <td>0.507401</td>\n",
       "      <td>0.124685</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n0028cb874439df8</th>\n",
       "      <td>0.129547</td>\n",
       "      <td>0.247748</td>\n",
       "      <td>0.486497</td>\n",
       "      <td>-0.633783</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  prediction_w_risk  prediction  preds_model_medium_LGBM_cfg1  \\\n",
       "id                                                                              \n",
       "n000777698096000           0.026383    0.141486                      0.475515   \n",
       "n0009793a3b91c27           0.766374    0.623966                      0.509630   \n",
       "n00099ccd6698ab0           0.906050    0.725224                      0.517587   \n",
       "n0019e36bbb8702b           0.711680    0.552808                      0.507401   \n",
       "n0028cb874439df8           0.129547    0.247748                      0.486497   \n",
       "\n",
       "                  preds_model_medium_LGBM_cfg1_neutral_riskiest_50  \\\n",
       "id                                                                   \n",
       "n000777698096000                                         -1.015812   \n",
       "n0009793a3b91c27                                          0.296737   \n",
       "n00099ccd6698ab0                                          0.562613   \n",
       "n0019e36bbb8702b                                          0.124685   \n",
       "n0028cb874439df8                                         -0.633783   \n",
       "\n",
       "                  target_nomi_20  \n",
       "id                                \n",
       "n000777698096000            0.25  \n",
       "n0009793a3b91c27            0.50  \n",
       "n00099ccd6698ab0            0.00  \n",
       "n0019e36bbb8702b            0.50  \n",
       "n0028cb874439df8            0.50  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "validation_data[[\"prediction_w_risk\", \"prediction\", \"preds_model_medium_LGBM_cfg1\", \"preds_model_medium_LGBM_cfg1_neutral_riskiest_50\", \"target_nomi_20\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50    269876\n",
       "0.25    107918\n",
       "0.75    107918\n",
       "1.00     26980\n",
       "0.00     26966\n",
       "Name: target_nomi_20, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "validation_data[\"target_nomi_20\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression to Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def val2class(x):\n",
    "    if 0 <= x < 0.125:\n",
    "        return 0\n",
    "    elif 0.125 <= x < 0.375:\n",
    "        return 1\n",
    "    elif 0.375 <= x < 0.625:\n",
    "        return 2\n",
    "    elif 0.625 <= x < 0.875:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# model as multiclass classification\n",
    "y_class = validation_data[\"target_nomi_20\"].apply(val2class).values.reshape(-1, 1)\n",
    "y_pred = validation_data[\"prediction_w_risk\"].apply(val2class).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539658, 1)\n",
      "(539658, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_class.shape)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "ohe = OneHotEncoder()\n",
    "y_class = ohe.fit_transform(y_class).toarray()\n",
    "y_pred = ohe.transform(y_pred).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539658, 5)\n",
      "(539658, 5)\n"
     ]
    }
   ],
   "source": [
    "print(y_class.shape)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5125012650278737"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "roc_auc_score(y_class, y_pred, multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model_clf_configs = {\n",
    "    \"LGBM_cfg1\": {\n",
    "        \"n_estimators\": 2000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"max_depth\": 5,\n",
    "        \"num_leaves\": 2 ** 5,\n",
    "        \"colsample_bytree\": 0.1\n",
    "    },\n",
    "    \"LGBM_cfg2\": {\n",
    "        \"n_estimators\": 3000,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"max_depth\": 5,\n",
    "        \"num_leaves\": 2 ** 5,\n",
    "        \"colsample_bytree\": 0.1\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model_clf_config = \"LGBM_cfg1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model 'model_clf_small_LGBM_cfg1'\n",
      "model not found, creating new one\n",
      "⠋\n",
      "✔ Training model\n"
     ]
    }
   ],
   "source": [
    "model_name = \"model_clf_{}_{}\".format(feature_set, model_clf_config)\n",
    "print(f\"Checking for existing model '{model_name}'\")\n",
    "model = load_model(model_name)\n",
    "if not model:\n",
    "    print(f\"model not found, creating new one\")\n",
    "    params = model_clf_configs[model_clf_config]\n",
    "\n",
    "    if model_clf_config.startswith(\"LGBM\"):\n",
    "        model = LGBMClassifier(**params)\n",
    "    else:\n",
    "        print(\"Not support\")\n",
    "    # elif model_config.startswith(\"RF\"):\n",
    "    #     model = RandomForestRegressor(**params)\n",
    "    # else:\n",
    "    #     model = XGBRegressor(**params)\n",
    "\n",
    "    # train on all of train and save the model so we don't have to train next time\n",
    "    spinner.start('Training model')\n",
    "    y = training_data[TARGET_COL].values.reshape(-1, 1)\n",
    "    ohe = OneHotEncoder()\n",
    "    y = ohe.fit_transform(y).toarray()\n",
    "    \n",
    "    model = OneVsRestClassifier(model).fit(training_data.filter(like='feature_', axis='columns'), y)#.predict(X)\n",
    "    # model.fit(training_data.filter(like='feature_', axis='columns'), y)\n",
    "    print(f\"saving new model: {model_name}\")\n",
    "    save_model(model, model_name)\n",
    "    spinner.succeed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "y = validation_data[TARGET_COL].values.reshape(-1, 1)\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# model_expected_features = model.booster_.feature_name()\n",
    "y_pred = model.predict(validation_data.filter(like='feature_', axis='columns'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51803957892829"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "roc_auc_score(y, y_pred, multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spinner.start('Reading example validation predictions')\n",
    "# validation_preds = pd.read_parquet(f\"pred/validation_predictions_{model_name}_{current_round}.csv\")\n",
    "validation_preds = pd.read_csv(f\"pred/validation_predictions_{model_name}_{current_round}.csv\")\n",
    "validation_data[EXAMPLE_PREDS_COL] = validation_preds[\"prediction\"]\n",
    "spinner.succeed()\n",
    "\n",
    "# get some stats about each of our models to compare...\n",
    "# fast_mode=True so that we skip some of the stats that are slower to calculate\n",
    "# validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\n",
    "# print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())\n",
    "# print(validation_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "advanced_model_configs = {\n",
    "    \"LGBM_cfg1\": {\n",
    "        \"n_estimators\": 2000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"max_depth\": 5,\n",
    "        \"num_leaves\": 2 ** 5,\n",
    "        \"colsample_bytree\": 0.1\n",
    "    },\n",
    "    \"LGBM_cfg2\": {\n",
    "        \"n_estimators\": 3000,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"max_depth\": 5,\n",
    "        \"num_leaves\": 2 ** 5,\n",
    "        \"colsample_bytree\": 0.1\n",
    "    },\n",
    "    \"RF_cfg1\": {\n",
    "        \"n_estimators\": 1000,\n",
    "        \"criterion\": \"squared_error\",\n",
    "        \"max_depth\": 5,\n",
    "        \"min_samples_leaf\": 5\n",
    "    },\n",
    "    \"RF_cfg2\": {\n",
    "        \"n_estimators\": 2000,\n",
    "        \"criterion\": \"squared_error\",\n",
    "        \"max_depth\": 3,\n",
    "        \"min_samples_leaf\": 5\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "advanced_training_configs = {\n",
    "    \"FEATURE_SET\": \"medium\",\n",
    "    \"MODEL_CONFIG\": \"LGBM_cfg1\",\n",
    "    \"downsample_cross_val\": 20,\n",
    "    \"downsample_full_train\": 1,\n",
    "    \"model_selection_loop\": True,\n",
    "    \"model_config_name\": \"advanced_example_model\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_name = \"LGBM_cfg1\"\n",
    "model_params = advanced_model_configs[model_config_name]\n",
    "\n",
    "print(\"Entering model selection loop.  This may take awhile.\")\n",
    "if advanced_training_configs[\"model_selection_loop\"]:\n",
    "    model_config = {}\n",
    "    print('downloading training_data')\n",
    "\n",
    "    # keep track of some prediction columns\n",
    "    ensemble_cols = set()\n",
    "    pred_cols = set()\n",
    "\n",
    "    # pick some targets to use\n",
    "    possible_targets = [c for c in training_data.columns if c.startswith(\"target_\")]\n",
    "    # randomly pick a handful of targets\n",
    "    # this can be vastly improved\n",
    "    targets = [\"target_nomi_20\"]\n",
    "\n",
    "    # all the possible features to train on\n",
    "    feature_cols = [c for c in training_data if c.startswith(\"feature_\")]\n",
    "\n",
    "    \"\"\" do cross val to get out of sample training preds\"\"\"\n",
    "    cv = 3\n",
    "    train_test_zip = get_time_series_cross_val_splits(training_data, cv=cv, embargo=12)\n",
    "    # get out of sample training preds via embargoed time series cross validation\n",
    "    # optionally downsample training data to speed up this section.\n",
    "    print(\"entering time series cross validation loop\")\n",
    "    for split, train_test_split in enumerate(train_test_zip):\n",
    "        gc.collect()\n",
    "        print(f\"doing split {split+1} out of {cv}\")\n",
    "        train_split, test_split = train_test_split\n",
    "        train_split_index = training_data[ERA_COL].isin(train_split)\n",
    "        test_split_index = training_data[ERA_COL].isin(test_split)\n",
    "        downsampled_train_split_index = train_split_index[train_split_index].index[::advanced_training_configs[\"downsample_cross_val\"]]\n",
    "\n",
    "        # getting the per era correlation of each feature vs the primary target across the training split\n",
    "        print(\"getting feature correlations over time and identifying riskiest features\")\n",
    "        all_feature_corrs_split = training_data.loc[downsampled_train_split_index, :].groupby(ERA_COL).apply(\n",
    "            lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\n",
    "        # find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\n",
    "        # there are probably more clever ways to do this\n",
    "        riskiest_features_split = get_biggest_change_features(all_feature_corrs_split, 50)\n",
    "\n",
    "        print(f\"entering model training loop for split {split+1}\")\n",
    "        for target in targets:\n",
    "            model_name = f\"model_{target}\"\n",
    "            print(f\"model: {model_name}\")\n",
    "\n",
    "            # train a model on the training split (and save it for future use)\n",
    "            downsample_cross_val = advanced_training_configs[\"downsample_cross_val\"]\n",
    "            split_model_name = f\"model_{target}_split{split+1}cv{cv}downsample{downsample_cross_val}\"\n",
    "            split_model = load_model(split_model_name)\n",
    "            if not split_model:\n",
    "                print(f\"training model: {model_name}\")\n",
    "                split_model = LGBMRegressor(**model_params)\n",
    "                split_model.fit(training_data.loc[downsampled_train_split_index, feature_cols],\n",
    "                                training_data.loc[downsampled_train_split_index,\n",
    "                                                  [target]])\n",
    "                save_model(split_model, split_model_name)\n",
    "            # now we can predict on the test part of the split\n",
    "            model_expected_features = split_model.booster_.feature_name()\n",
    "            if set(model_expected_features) != set(feature_cols):\n",
    "                print(f\"New features are available! Might want to retrain model {split_model_name}.\")\n",
    "            print(f\"predicting {model_name}\")\n",
    "            training_data.loc[test_split_index, f\"preds_{model_name}\"] = \\\n",
    "                split_model.predict(training_data.loc[test_split_index, model_expected_features])\n",
    "\n",
    "            # do neutralization\n",
    "            print(\"doing neutralization to riskiest features\")\n",
    "            training_data.loc[test_split_index, f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n",
    "                df=training_data.loc[test_split_index, :],\n",
    "                columns=[f\"preds_{model_name}\"],\n",
    "                neutralizers=riskiest_features_split,\n",
    "                proportion=1.0,\n",
    "                normalize=True,\n",
    "                era_col=ERA_COL)[f\"preds_{model_name}\"]\n",
    "\n",
    "            # remember that we made all of these different pred columns\n",
    "            pred_cols.add(f\"preds_{model_name}\")\n",
    "            pred_cols.add(f\"preds_{model_name}_neutral_riskiest_50\")\n",
    "\n",
    "        print(\"creating ensembles\")\n",
    "        # ranking per era for all of our pred cols so we can combine safely on the same scales\n",
    "        training_data[list(pred_cols)] = training_data.groupby(ERA_COL).apply(\n",
    "            lambda d: d[list(pred_cols)].rank(pct=True))\n",
    "        # do ensembles\n",
    "        training_data[\"ensemble_neutral_riskiest_50\"] = sum(\n",
    "            [training_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n",
    "            pct=True)\n",
    "        training_data[\"ensemble_not_neutral\"] = sum(\n",
    "            [training_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n",
    "        training_data[\"ensemble_all\"] = sum([training_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n",
    "\n",
    "        ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n",
    "        ensemble_cols.add(\"ensemble_not_neutral\")\n",
    "        ensemble_cols.add(\"ensemble_all\")\n",
    "\n",
    "    \"\"\" Now get some stats and pick our favorite model\"\"\"\n",
    "    print(\"gathering validation metrics for out of sample training results\")\n",
    "    all_model_cols = list(pred_cols) + list(ensemble_cols)\n",
    "    # use example_col preds_model_target as an estimates since no example preds provided for training\n",
    "    # fast_mode=True so that we skip some of the stats that are slower to calculate\n",
    "    training_stats = validation_metrics(training_data, all_model_cols, example_col=\"preds_model_target\",\n",
    "                                        fast_mode=True)\n",
    "    print(training_stats[[\"mean\", \"sharpe\"]].sort_values(by=\"sharpe\", ascending=False).to_markdown())\n",
    "\n",
    "    # pick the model that has the highest correlation sharpe\n",
    "    best_pred_col = training_stats.sort_values(by=\"sharpe\", ascending=False).head(1).index[0]\n",
    "    print(f\"selecting model {best_pred_col} as our highest sharpe model in validation\")\n",
    "\n",
    "    \"\"\" Now do a full train\"\"\"\n",
    "    print(\"entering full training section\")\n",
    "    # getting the per era correlation of each feature vs the target across all of training data\n",
    "    print(\"getting feature correlations with target and identifying riskiest features\")\n",
    "    all_feature_corrs = training_data.groupby(ERA_COL).apply(\n",
    "        lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\n",
    "    # find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\n",
    "    riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\n",
    "\n",
    "    for target in targets:\n",
    "        gc.collect()\n",
    "        downsample_full_train = advanced_training_configs[\"downsample_full_train\"]\n",
    "        model_name = f\"model_{target}_downsample{downsample_full_train}\"\n",
    "        model = load_model(model_name)\n",
    "        if not model:\n",
    "            print(f\"training {model_name}\")\n",
    "            model = LGBMRegressor(**model_params)\n",
    "            # train on all of train, predict on val, predict on tournament\n",
    "            model.fit(training_data.iloc[::advanced_training_configs[\"downsample_full_train\"]].loc[:, feature_cols],\n",
    "                      training_data.iloc[::advanced_training_configs[\"downsample_full_train\"]][target])\n",
    "            save_model(model, model_name)\n",
    "        gc.collect()\n",
    "\n",
    "    model_config[\"feature_cols\"] = feature_cols\n",
    "    model_config[\"targets\"] = targets\n",
    "    model_config[\"best_pred_col\"] = best_pred_col\n",
    "    model_config[\"riskiest_features\"] = riskiest_features\n",
    "    print(f\"saving model config for {model_config_name}\")\n",
    "    save_model_config(model_config, model_config_name)\n",
    "else:\n",
    "    # load model config from previous model selection loop\n",
    "    print(f\"loading model config for {model_config_name}\")\n",
    "    model_config = load_model_config(model_config_name)\n",
    "    feature_cols = model_config[\"feature_cols\"]\n",
    "    targets = model_config[\"targets\"]\n",
    "    best_pred_col = model_config[\"best_pred_col\"]\n",
    "    riskiest_features = model_config[\"riskiest_features\"]\n",
    "\n",
    "\n",
    "\"\"\" Things that we always do even if we've already trained \"\"\"\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccdbd9f1158aa7f65b3bacd2e1517888ef68beedcf7918a09b2f53f24aa4d343"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
